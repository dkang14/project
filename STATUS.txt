David Kang and Sally He

We've made progress on several fronts since the prototype was due. For one, we have re-implemented the data processing code in our python script into an object oriented framework. This has made it easier for us to add more methods/functionality with regard to the data (previously, the data gathering file was just a collection of functions, which was extremely messy and had to be managed by the outside user). Both the input data and output data are now encapsulated into objects with methods that interface directly with the neural network. 

Having optimized the python code, we've made the decision to re-implement the entire program in c++. We have already finished the reimplementation of the neural network module, which we have uploaded to github. We have observed huge performance gains, and we think that it makes sense to defer the computationally expensive neural network algorithm to a language that handles it better, aka c++. 

Finally, we have started the process of setting up the program to run concurrently on aws. Rather than parallelize the internal workings of the program itself, we have decided to take a parameter sweep approach. Each instance of EC2 will run a separate copy of our program with different initial parameters. Our goal is to find the "best" model for our data given a wide range of possible customization. In particular, I'd like to test whether there are gains from making the neural network extremely large. Currently, due to resource constraints, we only use about 10 nodes in our network. I am curious if having a large number of nodes (say 1000) would improve the results. Our parameter sweep is intended to explore such possibilities. 
